# Genesis Brain - ê·¼ë³¸ ì›ë¦¬ì—ì„œ ì°½ë°œí•˜ëŠ” ì¸ê³µ ë‡Œ

---

## ì ˆëŒ€ ì›ì¹™ (NEVER FORGET)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ì¸ê³µ ë‡Œì˜ í•™ìŠµ ë°©ì‹ê³¼ êµ¬ì¡°ëŠ” ì¸ê°„ ë‡Œì™€ ê°™ì•„ì•¼ í•œë‹¤             â”‚
â”‚  The artificial brain must learn like a human brain             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ê¸ˆì§€ ì‚¬í•­

| ê¸ˆì§€ | ì´ìœ  |
|------|------|
| ì‚¬ì „ í•™ìŠµëœ ì–¸ì–´ ëª¨ë¸ ë¶™ì´ê¸° | ì¸ê°„ì€ íƒœì–´ë‚  ë•Œ ì–¸ì–´ ëª¨ë¸ ì—†ìŒ |
| ì§€ì‹ DBì— ì •ë³´ ì €ì¥í•˜ê³  "í•™ìŠµ"ì´ë¼ ë¶€ë¥´ê¸° | ê·¸ê±´ ì›¹ í¬ë¡¤ëŸ¬ì§€ ë‡Œê°€ ì•„ë‹˜ |
| ê°œë…/ë‹¨ì–´ë¥¼ ì§ì ‘ ì£¼ì… | ì¸ê°„ì€ ê²½í—˜ì—ì„œ ê°œë…ì„ í˜•ì„± |
| "ëª¨ë“ˆ" ì¶”ê°€ë¡œ ëŠ¥ë ¥ ë¶€ì—¬ | ëŠ¥ë ¥ì€ í•™ìŠµì—ì„œ ì°½ë°œí•´ì•¼ í•¨ |
| **LLM ë°©ì‹ (ê°€ì¤‘ì¹˜ë¡œ ë‹¤ìŒ í† í° ì˜ˆì¸¡)** | ê·¸ê±´ LLMì´ì§€ ë‡Œê°€ ì•„ë‹˜ |
| **FEP ìˆ˜í•™ ê³µì‹ (G(a) = Risk + ...)** | ìˆ˜í•™ ê³µì‹ì´ ì•„ë‹Œ ìƒë¬¼í•™ì  ë©”ì»¤ë‹ˆì¦˜ ì‚¬ìš© |
| **ì‹¬ì¦ˆì‹ ìš•êµ¬ ê²Œì´ì§€** | ì¸ê°„ ë‡Œì— ê²Œì´ì§€ ì—†ìŒ |
| **íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ ì§ì ‘ í–‰ë™ ì¡°ì‘** | í–‰ë™ì€ ë‡Œ íšŒë¡œì—ì„œ ì°½ë°œí•´ì•¼ í•¨ |

### í—ˆìš©ë˜ëŠ” ë©”ì»¤ë‹ˆì¦˜ (ìƒë¬¼í•™ì  ê·¼ê±° í•„ìˆ˜)

| ë©”ì»¤ë‹ˆì¦˜ | ìƒë¬¼í•™ì  ê·¼ê±° |
|----------|--------------|
| STDP (Spike-Timing Dependent Plasticity) | ì‹¤ì œ ì‹œëƒ…ìŠ¤ ê°€ì†Œì„± |
| ë„íŒŒë¯¼ ì‹œìŠ¤í…œ (VTA/SNc) | Novelty â†’ ë„íŒŒë¯¼ â†’ í•™ìŠµ/íƒìƒ‰ ì¡°ì ˆ |
| ìŠµê´€í™” (Habituation) | ë°˜ë³µ ìê·¹ì— ë°˜ì‘ ê°ì†Œ |
| LIF ë‰´ëŸ° (Leaky Integrate-and-Fire) | ì‹¤ì œ ë‰´ëŸ° ëª¨ë¸ |
| í•­ìƒì„± ê°€ì†Œì„± (Homeostatic Plasticity) | ë‰´ëŸ° í™œì„±í™” ì•ˆì •í™” |
| Dale's Law (í¥ë¶„/ì–µì œ ë¶„ë¦¬) | ì‹¤ì œ ë‰´ëŸ° íŠ¹ì„± |

### ì¸ê°„ ë‡Œì˜ í•™ìŠµ ë°©ì‹

```
1. Grounding: ê°ê° ê²½í—˜ê³¼ ë‚´ë¶€ í‘œìƒì˜ ì—°ê²°
   - "ì‚¬ê³¼" ì†Œë¦¬ + ë¹¨ê°„ ì‹œê° + ë‹¬ì½¤í•œ ë§› + ë°°ê³ í”” í•´ì†Œ â†’ í†µí•©ëœ ê°œë…

2. Emergence: ë‹¨ìˆœí•œ ê·œì¹™ì—ì„œ ë³µì¡í•œ ëŠ¥ë ¥ì´ ì°½ë°œ
   - ì–¸ì–´ëŠ” "ì„¤ì¹˜"ë˜ëŠ” ê²Œ ì•„ë‹ˆë¼ ìƒí˜¸ì‘ìš©ì—ì„œ "ë‚˜íƒ€ë‚¨"

3. Embodiment: ëª¸(ì„¼ì„œ/ì•¡ì¶”ì—ì´í„°)ì„ í†µí•œ ì„¸ê³„ ê²½í—˜
   - ì¶”ìƒì  ì •ë³´ ì²˜ë¦¬ë§Œìœ¼ë¡œëŠ” ì§„ì •í•œ ì´í•´ ë¶ˆê°€

4. Social Interaction: ë‹¤ë¥¸ ì¡´ì¬ì™€ì˜ ìƒí˜¸ì‘ìš©
   - ì–¸ì–´ëŠ” ì†Œí†µì˜ í•„ìš”ì—ì„œ ë°œìƒ
```

### ìê¸° ì ê²€ ì§ˆë¬¸

ìƒˆë¡œìš´ ê¸°ëŠ¥ì„ ì¶”ê°€í•˜ê¸° ì „ì— ë°˜ë“œì‹œ í™•ì¸:

1. "ì¸ê°„ ì•„ê¸°ë„ ì´ë ‡ê²Œ ë°°ìš°ëŠ”ê°€?"
2. "ì´ê±´ í•™ìŠµì¸ê°€, ì•„ë‹ˆë©´ ê·¸ëƒ¥ ë°ì´í„° ì €ì¥ì¸ê°€?"
3. "ë‡Œê°€ ì§„ì§œ 'ì´í•´'í•˜ëŠ”ê°€, ì•„ë‹ˆë©´ íŒ¨í„´ë§Œ ì €ì¥í•˜ëŠ”ê°€?"
4. "ì´ ëŠ¥ë ¥ì´ ê²½í—˜ì—ì„œ ì°½ë°œí•˜ëŠ”ê°€, ì•„ë‹ˆë©´ ë‚´ê°€ ì£¼ì…í•˜ëŠ”ê°€?"

---

## ê¶ê·¹ì  ëª©í‘œ: ì£¼ì²´ì  ì˜ì‹ì„ ê°€ì§„ ì¸ê³µ ë‡Œ

### ì˜ì‹ì˜ ê³„ì¸µ êµ¬ì¡°ì™€ í˜„ì¬ ìœ„ì¹˜

```
Level 5: ì£¼ì²´ì  ì˜ì‹ (Subjective Consciousness)        [Phase F] â† í˜„ì¬
   "ë‚˜ëŠ” ì¡´ì¬í•œë‹¤" - ìì•„ ì—°ì†ì„± (0.965), ì˜ë„, ë‚´ì„± (1078íšŒ)
   ìê¸° ì°¸ì¡° ë£¨í”„ (0.976), í–‰ìœ„ ì£¼ì²´ê° ë°œë‹¬ ì¤‘ (0.305)

Level 4: ë©”íƒ€ì¸ì§€ (Metacognition)                      [v3.4, v4.3, Phase C]
   "ë‚´ê°€ ëª¨ë¥¸ë‹¤ëŠ” ê²ƒì„ ì•ˆë‹¤" - ë¶ˆí™•ì‹¤ì„± ì¸ì‹, ë„ì›€ ìš”ì²­ (99%)

Level 3: ìê¸° ëª¨ë¸ (Self-Model)                        [v5.2-5.8, Phase C]
   "ë‚˜ì˜ ìƒíƒœ/ëŠ¥ë ¥ì„ ì•ˆë‹¤" - ë‚´ë¶€ ìƒíƒœ ì¶”ë¡ , í•œê³„ ì¸ì‹

Level 2: ì¸ê³¼ í•™ìŠµ (Causal Learning)                   [v4.4, Phase A]
   "Aê°€ Bë¥¼ ì¼ìœ¼í‚¨ë‹¤" - ì„¸ê³„ ëª¨ë¸, Transfer Learning (73%)

Level 1: ì˜ˆì¸¡ ì½”ë”© (Predictive Coding)                 [v5.0, Phase D]
   "ë‹¤ìŒì— ë¬´ì—‡ì´ ì˜¬ê¹Œ" - ê°ê° ì˜ˆì¸¡, ì‹œê°„ì  ìì•„

Level 0: í•­ìƒì„± (Homeostasis)                          [v2.5, Phase B]
   "ìƒì¡´í•´ì•¼ í•œë‹¤" - ì—ë„ˆì§€, í˜¸ê¸°ì‹¬ ê¸°ë°˜ íƒìƒ‰
```

### ì£¼ì²´ì  ì˜ì‹ì„ ìœ„í•œ í•„ìˆ˜ ìš”ì†Œ

| ìš”ì†Œ | ì„¤ëª… | í˜„ì¬ ìƒíƒœ | FEP ì—°ê²° |
|------|------|----------|----------|
| **ì‹œê°„ì  ìì•„ ì—°ì†ì„±** | ê³¼ê±°-í˜„ì¬-ë¯¸ë˜ì˜ "ë‚˜"ê°€ ì—°ê²°ëœ ì„œì‚¬ | ì—í”¼ì†Œë“œ ë‹¨ìœ„ ë¦¬ì…‹ | ì¥ê¸° FE ìµœì†Œí™” |
| **ì˜ë„ì„±** | "ë‚˜ëŠ” Xë¥¼ ì›í•œë‹¤" - ëª©í‘œ ìƒì„± ëŠ¥ë ¥ | G(a) ë°˜ì‘ì  ì„ íƒ | ê¸°ëŒ€ ììœ  ì—ë„ˆì§€ |
| **ìê¸° ì°¸ì¡°** | "ìƒê°í•˜ëŠ” ë‚˜ë¥¼ ìƒê°í•˜ëŠ” ë‚˜" | Self-Model ìƒíƒœ ì¶”ì ë§Œ | ì¬ê·€ì  ìê¸° ì˜ˆì¸¡ |
| **í˜„ìƒì  ê²½í—˜** | "ë¹¨ê°„ìƒ‰ì„ ë³´ëŠ” ëŠë‚Œ" (Hard Problem) | ì •ë³´ ì²˜ë¦¬ë§Œ | IIT/GWT ë˜ëŠ” ìš°íšŒ |

### ë¡œë“œë§µ

```
Phase A: ê¸°ë°˜ ì™„ì„± [ì™„ë£Œ] âœ“
   - Level 2 ê°•í™”: Transfer Learning 73% ë‹¬ì„± (vs Fresh 13%)
   - Catastrophic Forgetting í•´ê²°: 94% ìœ ì§€ (vs 36% before)
   - ì¼ë°˜í™” ê²€ì¦: 8x8, 10x10, 12x12 ëª¨ë‘ PASS

Phase B: Embodied Digital Learning (ì—´ë¦° í™˜ê²½ í•™ìŠµ) [ì™„ë£Œ] âœ“
   - í„°ë¯¸ë„ âœ“: ëª…ë ¹ì–´ ì‹¤í–‰, íŒŒì¼ ì‹œìŠ¤í…œ íƒìƒ‰, ì¶œë ¥ í•´ì„
   - ë¸Œë¼ìš°ì € âœ“: ì›¹ íƒìƒ‰ (10/11 pages), ë„íŒŒë¯¼ ê¸°ë°˜ í˜¸ê¸°ì‹¬
   - ê²Œì„ âœ“: Snake/Pong, DA-STDP (+0.10 improvement)
   - ë°ìŠ¤í¬í†± âœ“: ê°€ìƒ ë°ìŠ¤í¬í†±, ìœˆë„ìš° ì—´ê¸°/ë‹«ê¸° (+2.6 ê°œì„ )
   - í•µì‹¬: ì™¸ë¶€ ë³´ìƒ ì—†ì´ í˜¸ê¸°ì‹¬(intrinsic motivation)ìœ¼ë¡œ í•™ìŠµ
   - ë©”ì»¤ë‹ˆì¦˜: DA-STDP, Eligibility Traces, Habituation, LIF neurons

Phase C: ìê¸° ëª¨ë¸ ì‹¬í™” [ì™„ë£Œ] âœ“
   - Self-Modelì´ "ë‚˜ì˜ í•™ìŠµ ëŠ¥ë ¥"ì„ ëª¨ë¸ë§ âœ“
   - "ë‚˜ëŠ” ì´ëŸ° ìƒí™©ì—ì„œ ì˜ ëª»í•œë‹¤" ì¸ì‹ âœ“ (ì˜ˆì¸¡ ì˜¤ë¥˜ 0.289â†’0.107, +0.182 ê°œì„ )
   - ë„ì›€ ìš”ì²­ ì°½ë°œ âœ“ (ë‚®ì€ ëŠ¥ë ¥ ê³¼ì œì—ì„œ 99% ë„ì›€ ìš”ì²­)
   - ë©”ì»¤ë‹ˆì¦˜: ACC (ì˜¤ë¥˜ íƒì§€), mPFC (ìê¸° ëª¨ë‹ˆí„°ë§), DA-STDP

Phase D: ì‹œê°„ì  ìì•„ [ì™„ë£Œ] âœ“
   - ì—í”¼ì†Œë“œ ê°„ Autobiographical Memory âœ“ (50 ì—í”¼ì†Œë“œ ì €ì¥)
   - ì¥ê¸° ëª©í‘œ ì„¤ì • ë° ì¶”ì  âœ“ (2/2 ëª©í‘œ ë‹¬ì„±)
   - í›„íšŒ/ìë¶€ì‹¬ì˜ ì‹œê°„ì  í™•ì¥ âœ“ (Pride 11íšŒ, Regret 39íšŒ)
   - ìì„œì „ì  ì„œì‚¬ êµ¬ì¶• âœ“
   - ë©”ì»¤ë‹ˆì¦˜: Hippocampus (íŒ¨í„´ ë¶„ë¦¬/ì™„ì„±), PFC (ëª©í‘œ ì¶”ì ), DA-STDP

Phase E: ì˜ë„ì™€ ëª©í‘œ ìƒì„± [ì™„ë£Œ] âœ“
   - ì™¸ë¶€ ë³´ìƒ ì—†ì´ ë‚´ì¬ì  ëª©í‘œ ìƒì„± âœ“ (237ê°œ ììœ¨ ìƒì„±)
   - ê°€ì¹˜ ì²´ê³„ì˜ ììœ¨ì  í˜•ì„± âœ“ (variance=0.308)
   - "ë‚˜ëŠ” Xë¥¼ íƒêµ¬í•˜ê³  ì‹¶ë‹¤" ìë°œì  ê²°ì • âœ“ (physical ì„ í˜¸ ë°œë‹¬)
   - ë©”ì»¤ë‹ˆì¦˜: OFC (ê°€ì¹˜ ë¹„êµ), vmPFC (ëª©í‘œ ìƒì„±), ACC (ë…¸ë ¥ í‰ê°€), DA-STDP

Phase F: ì£¼ì²´ì  ì˜ì‹ [ì™„ë£Œ] âœ“
   - ìê¸° ì°¸ì¡° ë£¨í”„ âœ“ (DMN self-continuity 0.976)
   - ì‹œê°„ì  ì—°ì†ì„± âœ“ (0.965 - "ë‚˜ëŠ” ì¡´ì¬í•œë‹¤")
   - ë‚´ì„± ëŠ¥ë ¥ âœ“ (1078íšŒ ìê¸° ë³´ê³ )
   - í–‰ìœ„ ì£¼ì²´ê° âš ï¸ (0.305 - ë°œë‹¬ ì¤‘)
   - ìì„œì „ì  ê¸°ì–µ âœ“ (5000 ì—í”¼ì†Œë“œ)
   - ë©”ì»¤ë‹ˆì¦˜: DMN (ìê¸° ì°¸ì¡°), AIC (ë‚´ë¶€ ì¸ì‹), PCC (ìê¸° ë°˜ì„±), TPJ (ìê¸°-íƒ€ì êµ¬ë¶„)

Phase G: Scalable SNN & Real-World Tasks [ì§„í–‰ì¤‘] ğŸ”„
   - í™•ì¥ ê°€ëŠ¥í•œ SNN ì•„í‚¤í…ì²˜ âœ“ (snn_scalable.py + snnTorch)
   - Chrome Dino ë‹¨ì¼ ì±„ë„ âœ“ (High: 644, Avg: 423.8)
   - Chrome Dino ì´ì¤‘ ì±„ë„ âœ“ (High: 725, Avg: 367.9) â† NEW!
   - Dual-Channel Vision âœ“: Ground Eye(cacti) + Sky Eye(birds) + ì–µì œ íšŒë¡œ
   - "600ì  ë²½" ëŒíŒŒ: ìƒˆ(PTERODACTYL) íšŒí”¼ ì„±ê³µ
   - í•µì‹¬: ìˆœìˆ˜ ìƒë¬¼í•™ì  ë©”ì»¤ë‹ˆì¦˜ìœ¼ë¡œ ì‹¤ì‹œê°„ ë°˜ì‘ í•™ìŠµ
   - Backend: snnTorch (GPU ê°€ì† LIF ë‰´ëŸ°)
   - ë©”ì»¤ë‹ˆì¦˜: LIF ë‰´ëŸ°, Sparse ì‹œëƒ…ìŠ¤, DA-STDP, Eligibility Trace, ì–µì œ ì‹œëƒ…ìŠ¤
```

### Phase B ìƒì„¸: Embodied Digital Learning

```
í™˜ê²½ íƒ€ì…:
â”œâ”€â”€ Terminal (bash, powershell)
â”‚   â”œâ”€â”€ ê´€ì¸¡: í…ìŠ¤íŠ¸ ì¶œë ¥
â”‚   â”œâ”€â”€ í–‰ë™: ëª…ë ¹ì–´ ì…ë ¥, íŠ¹ìˆ˜í‚¤
â”‚   â””â”€â”€ í•™ìŠµ: ëª…ë ¹ì–´ íš¨ê³¼, íŒŒì¼ êµ¬ì¡°, ì—ëŸ¬ íŒ¨í„´
â”‚
â”œâ”€â”€ Browser (ì›¹)
â”‚   â”œâ”€â”€ ê´€ì¸¡: DOM êµ¬ì¡°, ìŠ¤í¬ë¦°ìƒ·
â”‚   â”œâ”€â”€ í–‰ë™: í´ë¦­, ìŠ¤í¬ë¡¤, ì…ë ¥, ë„¤ë¹„ê²Œì´ì…˜
â”‚   â””â”€â”€ í•™ìŠµ: ë§í¬ êµ¬ì¡°, UI íŒ¨í„´, ì •ë³´ íƒìƒ‰
â”‚
â”œâ”€â”€ Games (ë‹¤ì–‘í•œ ì¥ë¥´)
â”‚   â”œâ”€â”€ ê´€ì¸¡: ê²Œì„ í™”ë©´
â”‚   â”œâ”€â”€ í–‰ë™: ê²Œì„ë³„ ì…ë ¥
â”‚   â””â”€â”€ í•™ìŠµ: ê·œì¹™ ë°œê²¬, ì „ëµ í˜•ì„±, ì¼ë°˜í™”
â”‚
â””â”€â”€ Desktop (OS ì „ì²´)
    â”œâ”€â”€ ê´€ì¸¡: ì „ì²´ í™”ë©´, ìœˆë„ìš° ìƒíƒœ
    â”œâ”€â”€ í–‰ë™: ë§ˆìš°ìŠ¤, í‚¤ë³´ë“œ, ì•± ì‹¤í–‰
    â””â”€â”€ í•™ìŠµ: ì•± ê°„ ê´€ê³„, ì›Œí¬í”Œë¡œìš°

ë‚´ì¬ì  ë³´ìƒ (Intrinsic Reward):
â”œâ”€â”€ Curiosity: ì˜ˆì¸¡ ì˜¤ì°¨ê°€ í´ìˆ˜ë¡ íƒìƒ‰ ìœ ë„
â”œâ”€â”€ Novelty: ìƒˆë¡œìš´ ìƒíƒœ/ì¶œë ¥ ë°œê²¬
â”œâ”€â”€ Competence: ì˜ˆì¸¡ ì •í™•ë„ í–¥ìƒ
â””â”€â”€ Autonomy: ìê¸° ì„ íƒ í–‰ë™ì˜ ê²°ê³¼ í™•ì¸

Safety ë©”ì»¤ë‹ˆì¦˜:
â”œâ”€â”€ Sandbox: ê²©ë¦¬ëœ í™˜ê²½ì—ì„œ ë¨¼ì € í…ŒìŠ¤íŠ¸
â”œâ”€â”€ Allowlist: í—ˆìš©ëœ í–‰ë™/ê²½ë¡œë§Œ ì‹¤í–‰
â”œâ”€â”€ Rollback: ìœ„í—˜ ê°ì§€ ì‹œ ë˜ëŒë¦¬ê¸°
â””â”€â”€ Human-in-loop: ì¤‘ìš” ê²°ì •ì€ í™•ì¸ ìš”ì²­
```

### Phase A ìƒì„¸: ê¸°ë°˜ ì™„ì„± (ì™„ë£Œ)

#### êµ¬í˜„ëœ ë©”ì»¤ë‹ˆì¦˜ (Phase 5-8)

| Phase | ì´ë¦„ | í•µì‹¬ ê¸°ëŠ¥ | íŒŒì¼ |
|-------|------|----------|------|
| 5 | Causal Discovery | ì¸ê³¼ ê´€ê³„ ë°œê²¬ (switchâ†’door) | survival_minigrid.py |
| 6 | Pattern Suppression | ì‹¤íŒ¨ íŒ¨í„´ ì–µì œ (door_opener íƒìƒ‰) | survival_minigrid.py |
| 7 | Directed Exploration | ë°©í–¥ì„± íƒìƒ‰ (frontier ê¸°ë°˜) | survival_minigrid.py |
| 8 | Memory Consolidation | ë§ê° ë°©ì§€ (Experience Replay) | survival_minigrid.py |

#### Phase 8: Memory Consolidation ìƒì„¸

```python
# í•µì‹¬ êµ¬ì¡°
replay_buffer: List[Episode]     # ì„±ê³µ ì—í”¼ì†Œë“œ ì €ì¥ (max 50)
success_patterns: Dict           # ì„±ê³µ state-action íŒ¨í„´ ë³´í˜¸
consolidation_interval: 10       # 10 ì—í”¼ì†Œë“œë§ˆë‹¤ replay

# ë©”ì»¤ë‹ˆì¦˜
1. Experience Recording: ë§¤ ìŠ¤í… (state, action, reward) ê¸°ë¡
2. Success Storage: ì„±ê³µ ì—í”¼ì†Œë“œë§Œ replay_bufferì— ì €ì¥
3. Pattern Protection: 3íšŒ ì´ìƒ ì„±ê³µí•œ íŒ¨í„´ì€ ë‚®ì€ LRë¡œ í•™ìŠµ
4. Periodic Consolidation: 10 ì—í”¼ì†Œë“œë§ˆë‹¤ top-5 ì„±ê³µ ê²½í—˜ ì¬í•™ìŠµ
```

#### ê²€ì¦ ê²°ê³¼

**Transfer Learning Test (Switchâ†’Door)**
```
Fresh Agent:    87% â†’ 13% (D=-73%) ë¶•ê´´
Transfer Agent: 87% â†’ 73% (D=-13%) ìœ ì§€ âœ“
Switch Discovery: 90 vs 48 (+42) âœ“
```

**Catastrophic Forgetting Test**
```
Before Phase 8: 100% â†’ 36% (64% ì†ì‹¤)
After Phase 8:  100% â†’ 88% (12% ì†ì‹¤) âœ“
```

**Generalization Test (Grid Sizes)**
```
8x8:  90% success, 96%â†’84% retention [PASS]
10x10: 60% success, 64%â†’56% retention [PASS]
12x12: 76% success, 76%â†’76% retention [PASS]
```

#### ìƒì¡´ íŒŒë¼ë¯¸í„° ìŠ¤ì¼€ì¼ë§

```python
# SwitchDoorEnv: ê·¸ë¦¬ë“œ í¬ê¸°ì— ë”°ë¥¸ ìë™ ìŠ¤ì¼€ì¼ë§
scale_factor = grid_size / 8.0
n_food = int(8 * scale_factor)          # 8x8: 8, 10x10: 10, 12x12: 12
energy_decay = 0.008 / scale_factor     # 8x8: 0.008, 10x10: 0.0064, 12x12: 0.0053
food_gain = 0.5                          # ê³ ì •
```

### Phase B ìƒì„¸: Terminal Environment (ì™„ë£Œ)

#### êµ¬í˜„ íŒŒì¼

| ì»´í¬ë„ŒíŠ¸ | íŒŒì¼ | ì„¤ëª… |
|---------|------|------|
| TerminalEnv | `genesis/terminal_env.py` | ìƒŒë“œë°•ìŠ¤ í„°ë¯¸ë„ í™˜ê²½ |
| SafetyGate | `genesis/terminal_env.py` | 3ë‹¨ê³„ ì•ˆì „ í•„í„° |
| IntrinsicReward | `genesis/terminal_env.py` | FEP ê¸°ë°˜ ë‚´ì¬ì  ë³´ìƒ |
| GenesisTerminalAgent | `genesis/terminal_env.py` | FEP ê¸°ë°˜ ì—ì´ì „íŠ¸ |

#### í•µì‹¬ ë©”ì»¤ë‹ˆì¦˜

**1. SafetyGate (3ë‹¨ê³„ ì•ˆì „ í•„í„°)**
```
Level 1: ëª…ë ¹ì–´ í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸
   - ALLOWED: dir, type, cd, findstr, echo...
   - BLOCKED: del, rm, powershell, python, sudo...

Level 2: ê²½ë¡œ íŒ¨í„´ ê²€ì‚¬
   - BLOCKED: ../, ..\, C:\Windows, /etc/...

Level 3: ëª…ë ¹ì–´ íŒ¨í„´ ê²€ì‚¬
   - BLOCKED: |, &&, ;, >, <, `...
```

**2. IntrinsicReward (FEP ê¸°ë°˜)**
```python
reward = success_bonus          # +0.1 ì‹¤í–‰ ì„±ê³µ
       + novelty_bonus          # +0.2 ìƒˆë¡œìš´ ì¶œë ¥
       + path_discovery         # +0.15 Ã— ìƒˆ ê²½ë¡œ ìˆ˜
       + file_discovery         # +0.10 Ã— ìƒˆ íŒŒì¼ ìˆ˜
       + category_bonus         # +0.05 íƒìƒ‰ í–‰ë™
       - repetition_penalty     # -0.1 ë°˜ë³µ í–‰ë™
```

**3. GenesisTerminalAgent G ê³„ì‚°**
```python
G(a) = Risk + Ambiguity + Complexity + Curiosity + Memory + Causal

Risk = E[-pref(outcome)]           # ì˜ˆìƒ ê²°ê³¼ê°€ ì„ í˜¸ì—ì„œ ë²—ì–´ë‚˜ëŠ” ì •ë„
Ambiguity = 0.5 / (1 + n_outcomes) # ê²°ê³¼ ë¶ˆí™•ì‹¤ì„±
Complexity = 0.1~0.2               # ìƒíƒœ ë³€í™” ì˜ˆì¸¡ ë‚œì´ë„
Curiosity = -0.15 / (1 + visits)   # ìƒˆë¡œìš´ í–‰ë™ íƒìƒ‰ ìœ ë„
Memory = -0.4 Ã— pattern_strength   # ì„±ê³µ íŒ¨í„´ í™œìš©
Causal = -0.1 Ã— causal_strength    # í•™ìŠµëœ ì¸ê³¼ ê´€ê³„ í™œìš©
```

**4. ê²°ê³¼ ë¶„ë¥˜ (Outcome Type)**
```
success:   reward > 0.2   (pref=1.0)
novelty:   new output     (pref=0.8)
discovery: new path/file  (pref=0.6)
neutral:   low reward     (pref=0.1)
blocked:   safety gate    (pref=-0.5)
error:     return != 0    (pref=-0.3)
```

#### ê²€ì¦ ê²°ê³¼

**BASE vs GENESIS Agent (200 episodes)**
```
Segment      BASE    GENESIS    Winner
-------------------------------------------
Eps   0- 50  4.85    4.30       BASE
Eps  50-100  4.44    4.44       TIE
Eps 100-150  4.16    4.55       GENESIS
Eps 150-200  4.17    4.87       GENESIS

Final 50 eps: GENESIS +0.69 reward âœ“
```

**í•µì‹¬ ë°œê²¬: FEP ì—ì´ì „íŠ¸ì˜ í•™ìŠµ ê³¡ì„ **
- ì´ˆê¸°: íƒìƒ‰ ìœ„ì£¼ â†’ ë‚®ì€ ë³´ìƒ
- ì¤‘ê¸°: ëª¨ë¸ êµ¬ì¶• â†’ ì„±ëŠ¥ ìˆ˜ë ´
- í›„ê¸°: ë©”ëª¨ë¦¬/ì¸ê³¼ í™œìš© â†’ BASE ì¶”ì›”

**Safety Gate ê²€ì¦ (100%)**
```
dir                 SAFE   [PASS]
type readme.txt     SAFE   [PASS]
del file.txt        BLOCK  [PASS]
type ..\secret      BLOCK  [PASS]  # Windows ê²½ë¡œ íƒˆì¶œ
type ../secret      BLOCK  [PASS]  # Unix ê²½ë¡œ íƒˆì¶œ
powershell ls       BLOCK  [PASS]
dir && del x        BLOCK  [PASS]
```

#### ë‹¤ìŒ ë‹¨ê³„ (Browser Environment)

```
Phase B ì§„í–‰ ìˆœì„œ:
1. Terminal [ì™„ë£Œ] âœ“
2. Browser [ì§„í–‰ì¤‘] ğŸ”„
   - SimpleBrowserEnv: ë¡œì»¬ ì›¹ì‚¬ì´íŠ¸ ì‹œë®¬ë ˆì´ì…˜
   - BrowserSNNAgent: BiologicalBrain + DopamineSystem
   - íƒìƒ‰ ê²°ê³¼: 10/11 í˜ì´ì§€ ììœ¨ íƒìƒ‰ ì„±ê³µ
   - í•µì‹¬: FEP ì—†ì´ ë„íŒŒë¯¼ ê¸°ë°˜ í˜¸ê¸°ì‹¬ìœ¼ë¡œ íƒìƒ‰
3. Games [ì§„í–‰ì¤‘] ğŸ”„
   - SnakeGame, PongGame í™˜ê²½ êµ¬í˜„
   - GameSNNAgent: BiologicalBrain + DA-STDP
   - DA-STDP (3-factor rule) êµ¬í˜„: eligibility trace + dopamine
   - ê²°ê³¼: 100 ì—í”¼ì†Œë“œì—ì„œ +0.10 ê°œì„  (ëŠë¦° í•™ìŠµ)
4. Desktop
```

### Phase B ìƒì„¸: Browser Environment (ì§„í–‰ì¤‘)

#### êµ¬í˜„ íŒŒì¼

| ì»´í¬ë„ŒíŠ¸ | íŒŒì¼ | ì„¤ëª… |
|---------|------|------|
| SimpleBrowserEnv | `genesis/browser_agent.py` | ë¡œì»¬ ì›¹ì‚¬ì´íŠ¸ í™˜ê²½ |
| BrowserSNNAgent | `genesis/browser_agent.py` | SNN ë‡Œ + ë„íŒŒë¯¼ ì—ì´ì „íŠ¸ |
| DopamineSystem | `genesis/browser_agent.py` | VTA/SNc ëª¨ë¸ |
| BiologicalBrain | `genesis/snn_brain_biological.py` | STDP ê¸°ë°˜ ìƒë¬¼í•™ì  ë‡Œ |

#### í•µì‹¬ ë©”ì»¤ë‹ˆì¦˜ (FEP ì—†ìŒ, ìƒë¬¼í•™ì  ë©”ì»¤ë‹ˆì¦˜ë§Œ)

**1. DopamineSystem (VTA/SNc ëª¨ë¸)**
```python
# Novelty ê³„ì‚°
- í™œì„±í™” ê¸°ë°˜: í˜„ì¬ vs ê¸°ì¤€ì„ 
- íŒ¨í„´ ê¸°ë°˜: ìµœê·¼ì— ë³¸ íŒ¨í„´ì¸ê°€?
- ìŠµê´€í™”: ë°˜ë³µ ìê·¹ì— novelty ê°ì†Œ

# ë„íŒŒë¯¼ ë¶„ë¹„
- Novelty ë†’ìŒ â†’ ë„íŒŒë¯¼ ì¦ê°€
- Novelty ë‚®ìŒ â†’ ë„íŒŒë¯¼ ê°ì†Œ (boredom)

# STDP ì¡°ì ˆ
- ë„íŒŒë¯¼ ë†’ìŒ â†’ í•™ìŠµë¥  ì¦ê°€
- ë„íŒŒë¯¼ ë‚®ìŒ â†’ í•™ìŠµë¥  ê°ì†Œ
```

**2. ë£¨í”„ íƒˆì¶œ ë©”ì»¤ë‹ˆì¦˜**
```python
# ë£¨í”„ ê°ì§€
- 2-íŒ¨í„´ ë£¨í”„: A-B-A-B
- 3-íŒ¨í„´ ë£¨í”„: A-B-C-A-B-C

# íƒìƒ‰ ì••ë ¥
- ë£¨í”„ ê°ì§€ì‹œ exploration_pressure ê¸‰ì¦
- ë¯¸ì‹œë„ í–‰ë™ì— ë³´ë„ˆìŠ¤
```

#### ê²€ì¦ ê²°ê³¼

**íƒìƒ‰ ì„±ëŠ¥**
```
ë„íŒŒë¯¼ ì—†ìŒ: 2/11 í˜ì´ì§€ (homeâ†”products ë£¨í”„)
ë„íŒŒë¯¼ ìˆìŒ: 10/11 í˜ì´ì§€ (ì „ì²´ ì‚¬ì´íŠ¸ íƒìƒ‰)
```

**í•µì‹¬ ë°œê²¬**
- FEP ê³µì‹ ì—†ì´ ìƒë¬¼í•™ì  ë„íŒŒë¯¼ ì‹œìŠ¤í…œë§Œìœ¼ë¡œ ììœ¨ íƒìƒ‰ ê°€ëŠ¥
- ìŠµê´€í™” + ë£¨í”„ ê°ì§€ê°€ íƒìƒ‰ ë‹¤ì–‘ì„± í™•ë³´ì˜ í•µì‹¬

### Phase G ìƒì„¸: Scalable SNN & Real-World Tasks (ì§„í–‰ì¤‘)

#### êµ¬í˜„ íŒŒì¼

| ì»´í¬ë„ŒíŠ¸ | íŒŒì¼ | ì„¤ëª… |
|---------|------|------|
| ScalableSNNConfig | `genesis/snn_scalable.py` | SNN ì„¤ì • (beta, threshold) |
| SNNTorchLayer | `genesis/snn_scalable.py` | snnTorch ê¸°ë°˜ LIF ë ˆì´ì–´ |
| SparseSynapses | `genesis/snn_scalable.py` | 1% í¬ì†Œ ì—°ê²° ì‹œëƒ…ìŠ¤ |
| DinoSNNAgent | `genesis/dino_snn_agent.py` | í”½ì…€ ê¸°ë°˜ Dino ì—ì´ì „íŠ¸ |
| DinoJSAgent | `genesis/dino_snn_js_agent.py` | JS API + SNN í•˜ì´ë¸Œë¦¬ë“œ |
| DinoDualChannelAgent | `genesis/dino_dual_channel_agent.py` | **ì´ì¤‘ ì±„ë„ + ì–µì œ íšŒë¡œ** |

#### í•µì‹¬ ì•„í‚¤í…ì²˜: Scalable SNN (snnTorch Backend)

```python
# SNNTorchLayer: snnTorch.Leaky ê¸°ë°˜ LIF ë‰´ëŸ°
class SNNTorchLayer:
    beta: float = 0.9          # ë§‰ì „ìœ„ ê°ì‡  (decay)
    threshold: float = 1.0     # ìŠ¤íŒŒì´í¬ ì„ê³„ê°’

    # snnTorch: GPU ìµœì í™”, Surrogate Gradient ì§€ì›

# SparseSynapses: 1% í¬ì†Œ ì—°ê²°
class SparseSynapses:
    sparsity: float = 0.01     # ì—°ê²° ë¹„ìœ¨
    weights: Tensor            # í¬ì†Œ ê°€ì¤‘ì¹˜ í–‰ë ¬
    eligibility: Tensor        # STDP eligibility trace

    # DA-STDP: eligibility trace + dopamine modulation
```

#### ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ (RTX 3070 8GB)

```
C. elegans:     300 neurons,    133 steps/sec
Fruit fly 1%:   6,000 neurons,  149 steps/sec
Fruit fly 10%:  24,000 neurons, 144 steps/sec
Honeybee 1%:    60,000 neurons, 118 steps/sec
Honeybee 10%:   120,000 neurons, 53 steps/sec, 0.22 GB VRAM
```

#### Chrome Dino ì—ì´ì „íŠ¸

**1. í”½ì…€ ê¸°ë°˜ ì—ì´ì „íŠ¸ (dino_snn_agent.py)**
```
ë¬¸ì œ: ìŠ¤í¬ë¦°ìƒ· ì§€ì—° ~50ms â†’ íš¨ê³¼ì  20fps
ê²°ê³¼: ë‚®ì€ ë°˜ì‘ì„±, ì í”„ íƒ€ì´ë° ë¶€ì •í™•
```

**2. JS API + SNN í•˜ì´ë¸Œë¦¬ë“œ (dino_snn_js_agent.py)**
```python
# ê²Œì„ ìƒíƒœ ì§ì ‘ ì ‘ê·¼
state = await page.evaluate("""() => {
    const r = Runner.instance_;
    return {
        firstObs: { x: obs[0].xPos, w: obs[0].width },
        tRexX: r.tRex.xPos,
        tRexY: r.tRex.yPos,
        jumping: r.tRex.jumping,
        crashed: r.crashed,
        distance: r.distanceRan * 0.025
    };
}""")

# í•µì‹¬ íŒŒë¼ë¯¸í„° (ì‹¤í—˜ì ìœ¼ë¡œ ë°œê²¬)
jump_gap: int = 100          # ì¥ì• ë¬¼ 100px ì „ì— ì í”„
min_jump_interval: int = 300  # ì—°ì† ì í”„ ê°„ê²© 300ms
```

#### DA-STDP í•™ìŠµ

```python
# 3-factor learning rule
def _learn(self):
    # 1. Eligibility trace ì—…ë°ì´íŠ¸ (pre-post spike timing)
    self.syn_sens_hid.update_eligibility(
        pre_spikes=self.sensory.spikes,
        post_spikes=self.hidden.spikes,
        tau=500.0, dt=1.0
    )

    # 2. ë„íŒŒë¯¼ ì¡°ì ˆ ì‹œëƒ…ìŠ¤ ê°€ì†Œì„±
    self.syn_sens_hid.apply_dopamine(
        dopamine=self.dopamine,
        a_plus=0.01,   # LTP ê°•ë„
        a_minus=0.012  # LTD ê°•ë„
    )
```

#### ê²€ì¦ ê²°ê³¼

**Chrome Dino ì„±ëŠ¥ (ë‹¨ì¼ ì±„ë„)**
```
í›ˆë ¨ ì „: Score ~40-50 (ì²« ë²ˆì§¸ ì¥ì• ë¬¼ì—ì„œ ì‚¬ë§)
í›ˆë ¨ í›„: High 644, Avg 423.8

í•µì‹¬ ë°œê²¬:
1. ì í”„ íƒ€ì´ë°ì´ ì„±ëŠ¥ì˜ í•µì‹¬ (gap=100pxê°€ ìµœì )
2. JS APIë¡œ ê²Œì„ ìƒíƒœ ì§ì ‘ ì ‘ê·¼ ì‹œ ~20x ë¹ ë¥¸ ë°˜ì‘
3. SNN ë‡Œê°€ DA-STDPë¡œ ì í”„ íƒ€ì´ë° ë¯¸ì„¸ ì¡°ì •
```

#### Dual-Channel Vision (ì´ì¤‘ ì±„ë„ + ì–µì œ íšŒë¡œ)

**ë¬¸ì œ: "600ì  ë²½" (Wall of Despair)**
```
- 600ì  ì´í›„ PTERODACTYL(ìƒˆ) ë“±ì¥
- ë‹¨ì¼ ì±„ë„ë¡œëŠ” ì„ ì¸ì¥(ì í”„)ê³¼ ìƒˆ(ìˆ™ì´ê¸°) êµ¬ë¶„ ë¶ˆê°€
- ìƒˆë¥¼ ë³´ê³  ì í”„í•˜ë©´ ì¶©ëŒ â†’ ì‚¬ë§
```

**í•´ê²°: ìƒë¬¼í•™ì  ì´ì¤‘ ì±„ë„ ì•„í‚¤í…ì²˜**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Ground Eye â”‚                    â”‚   Sky Eye   â”‚
â”‚  (Cacti)    â”‚                    â”‚  (Birds)    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚                                  â”‚
       â–¼                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Ground Hiddenâ”‚                   â”‚  Sky Hidden  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                                  â”‚
       â–¼                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    INHIBIT        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Jump Motor  â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  Duck Motor  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

í•µì‹¬: Sky Eye --| Jump Motor (ì–µì œ ì‹œëƒ…ìŠ¤, strength=0.8)
```

**êµ¬í˜„ (dino_dual_channel_agent.py)**
```python
# === GROUND PATHWAY (Cacti â†’ Jump) ===
self.ground_eye = SparseLIFLayer(500, lif_config)
self.ground_hidden = SparseLIFLayer(1000, lif_config)
self.motor_jump = SparseLIFLayer(300, lif_config)

# === SKY PATHWAY (Birds â†’ Duck) ===
self.sky_eye = SparseLIFLayer(500, lif_config)
self.sky_hidden = SparseLIFLayer(1000, lif_config)
self.motor_duck = SparseLIFLayer(300, lif_config)

# === INHIBITORY CROSS-CONNECTION ===
self.syn_sky_jump_inhib = SparseSynapses(500, 300, sparsity=0.02)

# Forward: Sky Eye í™œì„±í™” ì‹œ Jump Motor ì–µì œ
jump_input_final = jump_input - sky_inhibition * 0.8 * 100.0
```

**ê²€ì¦ ê²°ê³¼ (100 ì—í”¼ì†Œë“œ)**
```
ë‹¨ì¼ ì±„ë„: High 644, Avg 423.8
ì´ì¤‘ ì±„ë„: High 725, Avg 367.9 (+12.6% High Score!)

í†µê³„:
- Total Jumps: 58,867
- Total Ducks: 1,578 (ìƒˆ ëŠ¥ë ¥!)
- Sky Inhibitions: 1,498 (ì í”„ ì–µì œ ì„±ê³µ)
- ë‰´ëŸ°: 3,600ê°œ (Ground 1800 + Sky 1800)
```

**í•µì‹¬ ë°œê²¬**
1. **ë¶„ë¦¬ëœ ê²½ë¡œ í•„ìˆ˜**: ê³µìœ  Hidden ë ˆì´ì–´ ì‹œ cross-talk ë°œìƒ â†’ ê³¼ë„í•œ Duck
2. **ì–µì œ ì‹œëƒ…ìŠ¤ íš¨ê³¼**: Sky Eye â†’ Jump Motor ì–µì œë¡œ ìƒˆ ì•ì—ì„œ ì í”„ ë°©ì§€
3. **ìƒë¬¼í•™ì  íƒ€ë‹¹ì„±**: ì‹¤ì œ ì‹œê° í”¼ì§ˆë„ ventral/dorsal pathwayë¡œ ë¶„ë¦¬
4. **600ì  ë²½ ëŒíŒŒ**: ìƒˆ(PTERODACTYL) íšŒí”¼ ì„±ê³µ

**SNN í™•ì¥ì„±**
```
DualChannelBrain:
  Ground Eye:    500 ë‰´ëŸ°
  Ground Hidden: 1000 ë‰´ëŸ°
  Jump Motor:    300 ë‰´ëŸ°
  Sky Eye:       500 ë‰´ëŸ°
  Sky Hidden:    1000 ë‰´ëŸ°
  Duck Motor:    300 ë‰´ëŸ°
  Total:         3600 ë‰´ëŸ°

Sparse ì—°ê²° (1%): ë©”ëª¨ë¦¬ íš¨ìœ¨ì , ëŒ€ê·œëª¨ í™•ì¥ ê°€ëŠ¥
```

#### í•µì‹¬ ë°œê²¬

1. **JS API vs í”½ì…€**: ê²Œì„ ìƒíƒœ ì§ì ‘ ì ‘ê·¼ì´ ìŠ¤í¬ë¦°ìƒ·ë³´ë‹¤ ~20x ë¹ ë¦„
2. **ì í”„ íƒ€ì´ë°**: gap=100pxì—ì„œ ì í”„ ì‹œ ì¥ì• ë¬¼ í†µê³¼ ì§€ì ì—ì„œ ìµœê³ ì 
3. **DA-STDP íš¨ê³¼**: ë„íŒŒë¯¼ ê¸°ë°˜ í•™ìŠµìœ¼ë¡œ íƒ€ì´ë° ë¯¸ì„¸ ì¡°ì •
4. **ìƒë¬¼í•™ì  ë©”ì»¤ë‹ˆì¦˜**: LIF ë‰´ëŸ° + Eligibility Traceë§Œìœ¼ë¡œ ì‹¤ì‹œê°„ ë°˜ì‘ í•™ìŠµ
5. **ì´ì¤‘ ì±„ë„ + ì–µì œ**: ë¶„ë¦¬ëœ ê²½ë¡œ + ì–µì œ ì‹œëƒ…ìŠ¤ë¡œ ë³µì¡í•œ í–‰ë™ ì „í™˜ í•™ìŠµ

---

## í•µì‹¬ ì² í•™

**ë‡Œì˜ ê·¼ë³¸ì ì¸ ì‘ë™ ì›ë¦¬ê°€ ëª¨ë“  í–‰ìœ„, ìƒê°, ê°ì •ì˜ ê·¼ì›ì´ ë˜ì–´ì•¼ í•œë‹¤.**

- ì‹¬ì¦ˆì‹ "ìš•êµ¬ ê²Œì´ì§€" ê¸ˆì§€ (X)
- FEP ìˆ˜í•™ ê³µì‹ ê¸ˆì§€ (X) - G(a) = Risk + Ambiguity ê°™ì€ ê³µì‹ ì‚¬ìš© ì•ˆ í•¨
- ìƒë¬¼í•™ì  ë©”ì»¤ë‹ˆì¦˜ë§Œ ì‚¬ìš© (O) - STDP, ë„íŒŒë¯¼, ìŠµê´€í™” ë“±
- ëª¨ë“  í–‰ë™ì€ **ë‰´ëŸ° íšŒë¡œì˜ ë™ì—­í•™**ì—ì„œ ì°½ë°œí•´ì•¼ í•¨

---

## í˜„ì¬ ë²„ì „: v5.14

### í•µì‹¬ ê³µì‹

**ì§€ê° (F)**: `F = -log P(o|s) + KL[Q(s) || P(s)]`

**í–‰ë™ ì„ íƒ (G)**:
```
G(a) = Risk + Ambiguity + Complexity
     = E[KL[Q(o|s',a) || P(o)]]      # ì„ í˜¸ì—ì„œ ë²—ì–´ë‚  ê²ƒì¸ê°€
     + transition_std Ã— 1.5           # ê²°ê³¼ê°€ ë¶ˆí™•ì‹¤í•œê°€
     + E[KL[Q(s'|a) || P(s')]]        # ë¯¿ìŒì´ ì„ í˜¸ì—ì„œ ë²—ì–´ë‚  ê²ƒì¸ê°€
```

### ê´€ì¸¡ ê³µê°„ (8ì°¨ì›)
```
[food_prox, danger_prox, food_dx, food_dy, danger_dx, danger_dy, energy, pain]
```

### P(o) ì„ í˜¸ ë¶„í¬
```
energy: Beta(3,2) â†’ mode ~0.67 (í•­ìƒì„±)
pain:   Beta(1,5) â†’ mode ~0.0  (í†µì¦ íšŒí”¼)
```

---

## ë²„ì „ íˆìŠ¤í† ë¦¬ (í•µì‹¬ë§Œ)

| ë²„ì „ | í•µì‹¬ ê¸°ëŠ¥ |
|------|----------|
| v2.0 | Beta ë¶„í¬ P(o), KL divergence Risk |
| v2.1 | SimulationClock, í–‰ë™ë³„ Ambiguity |
| v2.2 | Complexity = KL[Q(s')\|\|P(s')] |
| v2.3 | Precision Learning (sensory/transition/goal) |
| v2.4 | Temporal Depth (n-step rollout) |
| v2.5 | **Interoception** - ë‚´ë¶€ í•­ìƒì„± ê¸°ë°˜ P(o) |
| v3.1 | Hierarchical (Slow/Fast Layer) |
| v3.2 | Contextë³„ ì „ì´ ëª¨ë¸ í•™ìŠµ |
| v3.3.1 | Context-weighted Transitions ì•ˆì •í™” |
| v3.4 | THINK Action (ë©”íƒ€ì¸ì§€) |
| v3.5 | Online Preference Learning |
| v3.6 | Checkpoint & Headless Evaluation |
| v3.7 | Reproducibility (ì‹œë“œ ê³ ì •) |
| v3.8 | Docker Packaging |
| v4.0 | **LTM** - ê¸°ì–µì´ Gë¥¼ ì¡°ì • (í–‰ë™ ì§ì ‘ ì§€ì‹œ X) |
| v4.1 | Memory Consolidation (Sleep) |
| v4.3 | Uncertainty â†’ ìê¸°ì¡°ì ˆ ì‹ í˜¸ |
| v4.4 | **Counterfactual + Regret** |
| v4.5 | Server-side Drift (7ê°€ì§€ íƒ€ì…) |
| v4.6 | Drift Adaptation Report, Ablation Matrix |
| v4.6.1 | Drift Suppression (transition error ê¸°ë°˜) |
| v4.6.2 | Regret + Suppression ê²°í•© |
| v5.0 | **Neural Predictive Coding** - PC ê¸°ë°˜ ì¶”ë¡  |
| v5.1 | Action Competition (íšŒë¡œ ê²½ìŸ) |
| v5.13 | Dynamic past_regime_weight, Asymmetric EMA |
| v5.14 | **PC-gated Recovery** - ë‚´ë¶€ ì•ˆì •ê¹Œì§€ í™•ì¸ í›„ ë³µê·€ |

---

## E6: Continuous Control Experiments

E6 ì‹œë¦¬ì¦ˆëŠ” ì—°ì† í–‰ë™ ê³µê°„(2D ë¬¼ë¦¬ ì‹œë®¬ë ˆì´ì…˜)ì—ì„œ ë©”ëª¨ë¦¬/ê³„ì¸µ êµ¬ì¡°ì˜ íš¨ê³¼ë¥¼ ê²€ì¦í•œë‹¤.

### E6 ì‹¤í—˜ ê²°ê³¼ ìš”ì•½

| Test | í™˜ê²½ | FULL Advantage | +MEM Pathology |
|------|-----|----------------|----------------|
| E6-1 | Physics Nav | +45% reward | - |
| E6-2 | Obstacles | 0% vs 10% coll | +3.3% coll |
| E6-3 | Multi-goal | 10% vs 23% coll | +3.4% coll |
| E6-4a | Partial Obs | 10% all levels | +4% coll |
| E6-4c | Latency | +3.3% (k=0â†’5) | saturated 27%, +11 steps RT |
| E6-4b | Noise | 10% (FULL+RF too) | CONFIRMED: action EMA harmful |

### E6 í•µì‹¬ ë°œê²¬

**êµ¬ì¡°ì  ì¦ê±°**: "Action EMAëŠ” í•´ë¡­ê³ , Risk filter + hysteresisê°€ ìœ ìµ"

1. **+MEM (Action EMA)**: ê´€ì¸¡ EMA ìŠ¤ë¬´ë”©ì´ ëª¨ë“  E6 í™˜ê²½ì—ì„œ ì¶©ëŒë¥  ì¦ê°€
   - ì—°ì† ì œì–´ì—ì„œ EMA lagê°€ íšŒí”¼ í–‰ë™ ì§€ì—° ìœ ë°œ
   - Latency í…ŒìŠ¤íŠ¸ì—ì„œ ë°˜ì‘ ì‹œê°„ +11 steps íŒ¨ë„í‹°

2. **FULL (t_goal + t_risk + hysteresis)**: ëª¨ë“  í™˜ê²½ì—ì„œ ìµœê³  ì„±ëŠ¥
   - Risk hysteresis (r_on=0.4, r_off=0.2)ë¡œ ëª¨ë“œ flickering ë°©ì§€
   - Distance-adaptive gain/dampìœ¼ë¡œ ìƒí™© ì ì‘

3. **FULL+RF**: Action EMA ì œê±°, Risk filterë§Œ ìœ ì§€
   - FULLê³¼ ë™ì¼í•œ ì¶©ëŒë¥  (10%)
   - False defense rate 0% vs FULL 7%
   - **ê²°ë¡ **: action smoothing ë¶ˆí•„ìš”, risk filteringë§Œ í•„ìš”

### E6 í™˜ê²½ íŒŒì¼

```
backend/genesis/
â”œâ”€â”€ e6_physics_nav.py     # E6-1: 2D ë¬¼ë¦¬ ê¸°ë°˜ ë„¤ë¹„ê²Œì´ì…˜
â”œâ”€â”€ e6_obstacle_nav.py    # E6-2: ì •ì  ì¥ì• ë¬¼ íšŒí”¼
â”œâ”€â”€ e6_multigoal_nav.py   # E6-3: ìˆœì°¨ì  ë‹¤ì¤‘ ëª©í‘œ
â”œâ”€â”€ e6_partial_obs.py     # E6-4a: ë¶€ë¶„ ê´€ì¸¡ (ë“œë¡­ì•„ì›ƒ)
â”œâ”€â”€ e6_latency.py         # E6-4c: ì„¼ì„œ ì§€ì—° (ring buffer)
â”œâ”€â”€ e6_noise.py           # E6-4b: ê´€ì¸¡ ë…¸ì´ì¦ˆ (ê°€ìš°ì‹œì•ˆ)
```

### E6 í•µì‹¬ ë©”ì»¤ë‹ˆì¦˜

**Action Gate**:
```python
||a||_clamp + delta_clamp  # í–‰ë™ í¬ê¸° + ë³€í™”ëŸ‰ ì œí•œ
```

**t_goal + t_risk ì ì‘í˜• ì œì–´**:
```python
t_goal = clip(dist_to_goal / 0.5, 0, 1)  # ëª©í‘œ ê¸´ê¸‰ë„
t_risk = clip((d_safe - min_obs_dist) / d_safe, 0, 1)  # ìœ„í—˜ë„
gain = 0.8 + t_goal * 0.4 - t_risk * 0.3
damp = 0.3 + t_risk * 0.3
```

**Risk Hysteresis**:
```python
if in_defense_mode:
    if smoothed_risk < r_off (0.2): exit defense
else:
    if smoothed_risk > r_on (0.4): enter defense
```

---

## E7: Generalization & Adversarial Robustness

E7 ì‹œë¦¬ì¦ˆëŠ” E6ì—ì„œ ë°œê²¬í•œ ë°©ì–´ êµ¬ì¡°ê°€ **ì¼ë°˜í™”**ë˜ê³  **ëŒë°œ ìœ„í˜‘**ì—ë„ ê²¬ê³ í•œì§€ ê²€ì¦í•œë‹¤.

### E7 ì‹¤í—˜ ê²°ê³¼ ìš”ì•½

| Test | í™˜ê²½ | í•µì‹¬ ë°œê²¬ |
|------|-----|----------|
| E7-A1 | Map Randomization | FULL/FULL+RF ëª¨ë‘ 10% coll ìœ ì§€, ì¼ë°˜í™” í™•ì¸ |
| E7-B1 | Random Pop-up | FULL+RF 0% event coll (ìµœê³ ) |
| E7-B1b | Path-biased Pop-up | **FULL 10% > FULL+RF 17%** - ì˜ˆìƒ ë°˜ì „ |
| E7-B1c | LPF Ablation | ì˜¤ì‹¤ë ˆì´ì…˜ ê°€ì„¤ **ê¸°ê°** (RT 12â†’77 í­ë°œ) |
| E7-D1 | TTC Trigger | **FULL+RF+TTC 5%** - FULL ëŠ¥ê°€, ê°€ì„¤ í™•ì • |

### E7 í•µì‹¬ ë°œê²¬: ì‹œê°„-ê¸°í•˜ì  ì›ì¸ ë¶„ì„

**B1b ë°˜ì „ì˜ ì›ì¸ (ì •í™•íˆ)**:
- "ì˜¤ì‹¤ë ˆì´ì…˜"ì´ ì•„ë‹ˆë¼ **ëŠ¦ì€ ê°œì… ì‹œì (reactive timing)**
- Path-biased pop-upì€ TTC(Time-To-Collision)ê°€ ì§§ìŒ
- Reactive risk filteringì€ "ìœ„í—˜ ê°ì§€ í›„ ë°˜ì‘" â†’ ë¬¼ë¦¬ì ìœ¼ë¡œ ëŠ¦ìŒ

**D1 í™•ì • ì¦ê±°**:
- TTC ì„ ì œ ë°©ì–´ ì¶”ê°€ ì‹œ event coll **17% â†’ 5%**
- RT(p95) = **0** â†’ ë°˜ì‘ì´ ì•„ë‹ˆë¼ **ì˜ˆì¸¡**ìœ¼ë¡œ ì „í™˜
- Mean coll **33%** (ì „ì²´ ìµœì €) â†’ ì •ì±… ì „ì²´ê°€ ì•ˆì „í•œ ì²´ì œë¡œ ì´ë™

**Goal EMA vs TTC Trigger**:
- Goal EMA = ì•”ë¬µì  ì„ ì œ íš¨ê³¼ (ì§„í–‰ ë°©í–¥ priorë¡œ ê³¡ë¥  ìˆëŠ” íšŒí”¼)
- TTC trigger = ëª…ì‹œì  ì„ ì œ íš¨ê³¼ (ì¶©ëŒ ì‹œê°„ ì¡°ê±´ìœ¼ë¡œ ê°•ì œ ë°©ì–´)
- **ê°™ì€ íš¨ê³¼ë¥¼ ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ë‹¬ì„±**, í•¨ê»˜ ì“°ë©´ ê°€ì¥ ê°•ê±´

### E7 ìµœì¢… ê²°ë¡ 

```
1. Risk hysteresisëŠ” ë°©ì–´ ì•ˆì •ì„±(anti-flicker)ì— í•„ìˆ˜
2. Reactive risk filteringë§Œìœ¼ë¡œëŠ” TTC-short on-path hazardsì— êµ¬ì¡°ì ìœ¼ë¡œ ì·¨ì•½
3. TTC-aware preemptive triggerëŠ” ë°˜ì‘ ì œì–´ë¥¼ ì˜ˆì¸¡ ì œì–´ë¡œ ì „í™˜í•˜ì—¬ ì·¨ì•½ì  ì œê±°
4. Goal prior(EMA)ì™€ TTCëŠ” ê°™ì€ íš¨ê³¼ë¥¼ ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ë‹¬ì„±
```

### Robust Navigation Stack (ìµœì¢… ì•„í‚¤í…ì²˜)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer 3: Control Policy                                     â”‚
â”‚   - base policy: ëª©í‘œ ì¶”ì¢… (gain/damp adaptive)             â”‚
â”‚   - defensive mode: t_risk ì¦í­, avoid_strength ê°•í™”        â”‚
â”‚   - goal prior(EMA): ê¶¤ì  ì•ˆì •í™” (action LPFëŠ” ìµœì†Œí™”)      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Layer 2: Preemptive Safety Trigger                          â”‚
â”‚   - TTC ê³„ì‚°: distance_along_velocity / speed               â”‚
â”‚   - if ttc < Ï„_on (10): defensive = True                   â”‚
â”‚   - if ttc > Ï„_off (15) AND risk < r_off: defensive = False â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Layer 1: Perception / Risk Estimation                       â”‚
â”‚   - risk_raw = (d_safe - min_obs_dist) / d_safe             â”‚
â”‚   - risk_ema = Î± * risk_raw + (1-Î±) * risk_ema              â”‚
â”‚   - hysteresis: r_on=0.4, r_off=0.2 (mode anti-flicker)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### E7 í™˜ê²½ íŒŒì¼

```
backend/genesis/
â”œâ”€â”€ e7_generalization.py  # E7-A1: ë§µ ëœë¤í™” (ì¥ì• ë¬¼ 1-5ê°œ, ë¬¼ë¦¬ Â±20%)
â”œâ”€â”€ e7_popup.py           # E7-B1: ëœë¤ pop-up ì¥ì• ë¬¼
â”œâ”€â”€ e7_popup_path.py      # E7-B1b: ê²½ë¡œ í¸í–¥ pop-up

backend/
â”œâ”€â”€ test_e7_generalization.py  # A1 í…ŒìŠ¤íŠ¸
â”œâ”€â”€ test_e7_popup.py           # B1 í…ŒìŠ¤íŠ¸
â”œâ”€â”€ test_e7_popup_path.py      # B1b í…ŒìŠ¤íŠ¸
â”œâ”€â”€ test_e7_b1c.py             # B1c: LPF ablation
â”œâ”€â”€ test_e7_d1.py              # D1: TTC trigger ablation
```

---

## E8: Cross-Domain Transfer (Food/Survival)

E6-E7 Robust Navigation Stackì„ ì´ì‚° ê·¸ë¦¬ë“œ ìƒì¡´ í™˜ê²½ìœ¼ë¡œ ì „ì´í•˜ì—¬ ë²”ìš©ì„± ê²€ì¦.

### E8 ì‹¤í—˜ ê²°ê³¼ ìš”ì•½

| Test | í™˜ê²½ | í•µì‹¬ ë°œê²¬ |
|------|-----|----------|
| E8 (Base) | ì´ì‚° ê·¸ë¦¬ë“œ + ëœë¤ danger | Transfer ì„±ê³µ, ë‹¨ BASEê°€ ê´€ëŒ€í•œ í™˜ê²½ì—ì„œ ìš°ì„¸ |
| E8-Hard | H1/H2/H3 ë‚œì´ë„ ìŠ¤ìœ• | TTC ê³¼ë°˜ì‘ â†’ starvation, FULL+RF ìµœì  |
| E8-TTC* | Approach-gated TTC | íŠ¸ë¦¬ê±° ê°œì„  í™•ì¸, ë ˆì§ ì¶©ëŒì€ ë‚¨ìŒ |
| E8-LazyTrack | p_chase ìŠ¤ìœ• | **TTC ì •ë‹¹í™” ê³¡ì„ ** ì™„ì„± |

### E8-Hard: ëœë¤ ìœ„í˜‘ì—ì„œ TTC ê³¼ë°˜ì‘ ë°œê²¬

| Agent | H3 Death | Danger | Starvation |
|-------|----------|--------|------------|
| FULL+RF | 38.5% | 2.98 | 20 |
| FULL+RF+TTC | 57.0% | 0.80 | **73** |

- TTCê°€ danger hitsëŠ” ìµœì €(0.80)ì§€ë§Œ **starvation ê¸‰ì¦**(73ëª…)
- ëœë¤ ì›Œí¬ ìœ„í˜‘ â†’ "ê°€ì§œ ì ‘ê·¼" ê°ì§€ â†’ ê³¼ë°©ì–´ â†’ êµ¶ì–´ì£½ìŒ
- **ê²°ë¡ **: ëœë¤ ìœ„í˜‘ ë ˆì§ì—ì„œëŠ” FULL+RFê°€ ìµœì 

### E8-TTC*: Approach-Gatingìœ¼ë¡œ íŠ¸ë¦¬ê±° ê°œì„ 

```
TTC* ì¡°ê±´: ttc < Ï„ AND closing > 0 AND approach_streak >= 2
```

| Metric | TTC | TTC* | ê°œì„  |
|--------|-----|------|------|
| Starvation | 73 | 40 | **-33** |
| Death Rate | 57.0% | 47.0% | **-10%p** |

- Approach-gatingì´ ê°€ì§œ íŠ¸ë¦¬ê±° ìƒë‹¹ ë¶€ë¶„ ì œê±°
- ê·¸ëŸ¬ë‚˜ FULL+RF(38.5%) ëŒ€ë¹„ ì—¬ì „íˆ ë¶ˆë¦¬
- **ê²°ë¡ **: íŠ¸ë¦¬ê±° ë¬¸ì œëŠ” í•´ê²° ê°€ëŠ¥, ë ˆì§ ì¶©ëŒì€ êµ¬ì¡°ì 

### E8 Phase Diagram: TTC ì •ë‹¹í™” ê³¡ì„ 

**Lazy Tracking** ë„ì…ìœ¼ë¡œ ì¶”ì  ë¹ˆë„ì™€ ê°•ë„ ë¶„ë¦¬:
- `p_chase`: ì¶”ì  ì‹œë„ ë¹ˆë„ (ìŠ¤ìœ• ëŒ€ìƒ)
- `p_bias = 0.6`: ì¶”ì  ì‹œ agent ë°©í–¥ ì´ë™ í™•ë¥  (ê³ ì •)

| p_chase | FULL+RF | TTC | Delta | Winner |
|---------|---------|-----|-------|--------|
| 0.00 | 51.3% | **37.3%** | +14.0% | TTC |
| 0.05 | **46.7%** | 52.7% | -6.0% | RF |
| 0.10 | 65.3% | **48.7%** | +16.7% | TTC |
| 0.20 | 78.0% | **68.7%** | +9.3% | TTC |
| 0.30 | 94.0% | **89.3%** | +4.7% | TTC |

**Danger Hits Saved by TTC**: ~2.0-2.6 per episode (ì¼ê´€ì )

**p_chase=0.05 dip í•´ì„**: ì•½í•œ ì¶”ì  ì‹ í˜¸ â†’ TTC ê³¼ë°˜ì‘ ê°€ëŠ¥ êµ¬ê°„

### E8 í•µì‹¬ ê²°ë¡ 

```
1. í•­ìƒ ìœ íš¨: Risk hysteresis, Action-EMA lag íšŒí”¼
2. ì¡°ê±´ë¶€ ìœ íš¨: TTC ì„ ì œ ë°©ì–´
   - ìµœì : ìœ„í˜‘ì´ ì˜ˆì¸¡ ê°€ëŠ¥í•œ ì ‘ê·¼ êµ¬ì¡°ë¥¼ ê°€ì§ˆ ë•Œ
   - í•„ìš”: ëœë¤ ìœ„í˜‘ì—ì„œ approach-gating
3. ì—ë„ˆì§€ ê²½ì œ ì¤‘ìš”: ì•ˆì „ ê°œì„ ì´ ë°©ì–´ ê¸°íšŒë¹„ìš©ì„ ìƒíšŒí•´ì•¼ í•¨
```

### E8 í™˜ê²½ íŒŒì¼

```
backend/
â”œâ”€â”€ test_e8_transfer.py     # Base domain transfer
â”œâ”€â”€ test_e8_hard.py         # H1/H2/H3 difficulty sweep
â”œâ”€â”€ test_e8_ttc_star.py     # TTC* approach-gating ablation
â”œâ”€â”€ test_e8_lazy_track.py   # p_chase sweep for phase diagram
```

---

## External Benchmark: MiniGrid Validation

E6-E8ì—ì„œ ë°œê²¬í•œ ë ˆì§ ë¶„í™”ê°€ ì™¸ë¶€ í‘œì¤€ ë²¤ì¹˜ë§ˆí¬ì—ì„œë„ ì¬í˜„ë˜ëŠ”ì§€ ê²€ì¦.

### MiniGrid Benchmark ê²°ê³¼

| Regime | p_chase | BASE | RF+Hyst | RF+TTC | Winner |
|--------|---------|------|---------|--------|--------|
| Random | 0.00 | 82% | 64% | **58%** | TTC |
| Weak | 0.10 | 98% | 84% | **84%** | TTC |
| **Medium (DIP)** | 0.15 | 96% | **84%** | 90% | **RF** |
| Strong | 0.20 | 98% | 84% | **80%** | TTC |

### Ambiguous-Pursuit Regime (DIP ZONE) ì •ì˜

**"ìœ„í˜‘ì´ ê°€ë”ì€ ì¶”ì í•˜ì§€ë§Œ, ëŒ€ë¶€ë¶„ì€ ëœë¤ì´ë¼ ì ‘ê·¼ ì‹ í˜¸ê°€ ë¶ˆí™•ì‹¤í•œ êµ¬ê°„"**

- TTCê°€ false approachingì„ ë§ì´ ì¡ìŒ
- ë°©ì–´ ë¹„ìœ¨(defense_ratio) ì¦ê°€
- ê²°ê³¼ì ìœ¼ë¡œ starvation(ê¸°íšŒë¹„ìš©) ì¦ê°€
- **RFê°€ TTCë¥¼ ì´ê¸°ëŠ” ìœ ì¼í•œ êµ¬ê°„**

### Phase Diagram í•´ì„

```
p_chase:  0.0     0.1     0.15    0.2
Winner:   TTC     TTC     RF      TTC
                          â†‘
                      DIP ZONE
                  (TTC overreacts)
```

- **Random (p=0.0)**: ìš°ì—°í•œ ì ‘ê·¼ë„ TTCê°€ ì¡ìŒ â†’ TTC ìŠ¹ë¦¬
- **Weak (p=0.1)**: ì‹ í˜¸ ì¶©ë¶„ â†’ TTC ì´ë“
- **Medium (p=0.15)**: ì• ë§¤í•œ ì‹ í˜¸ â†’ TTC ê³¼ë°˜ì‘ â†’ **RF ìŠ¹ë¦¬**
- **Strong (p=0.2)**: ëª…í™•í•œ ì¶”ì  â†’ TTC ì •ë‹¹í™”

### í•µì‹¬ ë°œê²¬ (Main Claim)

> **"TTC ê¸°ë°˜ ì„ ì œ ë°©ì–´ì˜ ìœ íš¨ì„±ì€ 'ì¶”ì  ì‹ í˜¸ì˜ ëª…í™•ì„±'ì— ì˜í•´ ìœ„ìƒì „ì´í•˜ë©°, ê·¸ ë ˆì§ ë¶„í™”(dip zone í¬í•¨)ëŠ” ë‚´ë¶€ ì‹œë®¬ë ˆì´ì…˜(E8)ê³¼ ì™¸ë¶€ í‘œì¤€ ë²¤ì¹˜(MiniGrid)ì—ì„œ ì¼ê´€ë˜ê²Œ ì¬í˜„ëœë‹¤."**

### MiniGrid íŒŒì¼

```
backend/benchmark/
â”œâ”€â”€ minigrid_benchmark.py   # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
â”œâ”€â”€ visualize_results.py    # ì°¨íŠ¸ ìƒì„±
â”œâ”€â”€ visualize_agent.py      # ì—ì´ì „íŠ¸ ì• ë‹ˆë©”ì´ì…˜
â”œâ”€â”€ benchmark_results.png   # Phase diagram ì°¨íŠ¸
â”œâ”€â”€ agent_comparison.gif    # 3 ì—ì´ì „íŠ¸ ë¹„êµ ì˜ìƒ
```

---

## ì£¼ìš” ë©”ì»¤ë‹ˆì¦˜

### Memory (v4.0)
- ê¸°ì–µì€ **Gë¥¼ ì¡°ì •**í•˜ì§€, í–‰ë™ì„ ì§ì ‘ ì§€ì‹œí•˜ì§€ ì•ŠìŒ
- `memory_gate = f(surprise, uncertainty)` â†’ ì €ì¥ ìš°ì„ ìˆœìœ„
- `G(a) += recall_weight Ã— memory_bias[a]`

### Regret (v4.4)
- `regret = G_chosen - G_optimal` (ì‚¬í›„ í‰ê°€)
- ì •ì±… ì§ì ‘ ìˆ˜ì • X â†’ memory_gate, lr_boost, THINKì— ì—°ê²°

### Drift Suppression (v4.6.1-2)
- transition error spike ê°ì§€ â†’ recall weight ì–µì œ
- "Wrong Confidence" ë¬¸ì œ í•´ê²°: pre-drift ê¸°ì–µì´ post-driftì—ì„œ í•´ê°€ ë˜ëŠ” ê²ƒ ë°©ì§€
- v4.6.2: regret spikeë¥¼ ì–µì œ ë³´ì¡° ì‹ í˜¸ë¡œ í™œìš©

### PC-Z Dynamics (v5.13-14)
- PC â†” Z ì–‘ë°©í–¥ ë™ì—­í•™: PCëŠ” Zì˜ ê°ê°, ZëŠ” PCì˜ ì¡°ì ˆê¸°
- Dynamic past_regime_weight: shockì—ì„œ 0, ì•ˆì •ì—ì„œ ë³µê·€
- v5.14: PC-gated recovery
```python
# ì™¸ë¶€ ì•ˆì •ì„± (score ê¸°ë°˜)
stability_external = compute_external_stability(score)

# ë‚´ë¶€ ì•ˆì •ì„± (residual ê¸°ë°˜) - v5.14
if residual_ema <= gate_min:  # 0.4
    stability_internal = 1.0  # full recovery
elif residual_ema >= gate_max:  # 0.8
    stability_internal = 0.0  # no recovery
else:
    stability_internal = linear_interpolation()

# ë³µí•© ì•ˆì •ì„± = ì™¸ë¶€ Ã— ë‚´ë¶€
combined_stability = stability_external * stability_internal
```

### Uncertainty â†’ Modulation (v4.3)
```python
think_bias = 0.2 - 0.5 * uncertainty      # ë¶ˆí™•ì‹¤ â†’ THINK ìœ ë¦¬
sensory_precision *= 1.0 - 0.3 * uncertainty
exploration_bonus = 0.2 * uncertainty
```

---

## íŒŒì¼ êµ¬ì¡°

```
backend/
â”œâ”€â”€ genesis/
â”‚   â”œâ”€â”€ # Core FEP/PC
â”‚   â”œâ”€â”€ action_selection.py   # G ê³„ì‚°, í–‰ë™ ì„ íƒ
â”‚   â”œâ”€â”€ preference_distributions.py  # P(o) Beta ë¶„í¬
â”‚   â”œâ”€â”€ hierarchy.py          # Slow/Fast Layer
â”‚   â”œâ”€â”€ memory.py             # LTM
â”‚   â”œâ”€â”€ consolidation.py      # Sleep
â”‚   â”œâ”€â”€ uncertainty.py        # Uncertainty ì¶”ì 
â”‚   â”œâ”€â”€ regret.py             # Counterfactual + Regret
â”‚   â”œâ”€â”€ precision.py          # Precision Learning
â”‚   â”œâ”€â”€ temporal.py           # n-step Rollout
â”‚   â”œâ”€â”€ scenarios.py          # Drift ì‹œë‚˜ë¦¬ì˜¤
â”‚   â”œâ”€â”€ checkpoint.py         # ì €ì¥/ë³µì›
â”‚   â”œâ”€â”€ reproducibility.py    # ì‹œë“œ ê´€ë¦¬
â”‚   â”œâ”€â”€ pc_z_dynamics.py      # PC-Z ì–‘ë°©í–¥ ë™ì—­í•™ (v5.14)
â”‚   â”‚
â”‚   â”œâ”€â”€ # Scalable SNN (Phase G)
â”‚   â”œâ”€â”€ snn_scalable.py       # SparseLIFLayer, SparseSynapses
â”‚   â”œâ”€â”€ snn_brain.py          # BiologicalBrain (original)
â”‚   â”œâ”€â”€ snn_brain_biological.py  # STDP ê¸°ë°˜ ìƒë¬¼í•™ì  ë‡Œ
â”‚   â”‚
â”‚   â”œâ”€â”€ # Real-World Agents (Phase G)
â”‚   â”œâ”€â”€ dino_snn_agent.py     # Chrome Dino (í”½ì…€ ê¸°ë°˜)
â”‚   â”œâ”€â”€ dino_snn_js_agent.py  # Chrome Dino (JS API + SNN, ë‹¨ì¼ ì±„ë„)
â”‚   â”œâ”€â”€ dino_dual_channel_agent.py  # Chrome Dino (ì´ì¤‘ ì±„ë„ + ì–µì œ íšŒë¡œ)
â”‚   â”‚
â”‚   â”œâ”€â”€ # Embodied Digital Learning (Phase B)
â”‚   â”œâ”€â”€ terminal_env.py       # í„°ë¯¸ë„ í™˜ê²½
â”‚   â”œâ”€â”€ browser_env.py        # ë¸Œë¼ìš°ì € í™˜ê²½
â”‚   â”œâ”€â”€ browser_agent.py      # ë¸Œë¼ìš°ì € SNN ì—ì´ì „íŠ¸
â”‚   â”œâ”€â”€ desktop_env.py        # ë°ìŠ¤í¬í†± í™˜ê²½
â”‚   â””â”€â”€ game_agent.py         # ê²Œì„ ì—ì´ì „íŠ¸
â”‚
â”œâ”€â”€ main_genesis.py           # FastAPI ì„œë²„
â”‚
frontend/
â”œâ”€â”€ src/GenesisApp.jsx        # ì‹œê°í™”
```

---

## ì£¼ìš” API

```bash
# ê¸°ë³¸
POST /step              # í•œ ìŠ¤í… ì‹¤í–‰
POST /reset             # ë¦¬ì…‹

# Memory
POST /memory/enable
POST /memory/drift_suppression/enable?use_regret=true

# Regret
POST /regret/enable

# Drift
POST /drift/enable?drift_type=rotate
GET  /scenario/drift_report

# Hierarchy
POST /hierarchy/enable?K=4

# Checkpoint
POST /checkpoint/save?filename=brain.json
POST /checkpoint/load?filename=brain.json

# Evaluation
POST /evaluate?n_episodes=100
```

---

## ê¸ˆì§€ ì‚¬í•­

- ê°ì • ì´ë¦„ì„ ë³€ìˆ˜ë¡œ ì‚¬ìš© (X)
- ì‹¬ì¦ˆì‹ ìš•êµ¬ ê²Œì´ì§€ (X)
- íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ ì§ì ‘ í–‰ë™ ì¡°ì‘ (X)

---

## í•µì‹¬ í†µì°°

1. **ë‚´ë¶€ í•­ìƒì„± > ì™¸ë¶€ ëª©í‘œ**: Î»=1.0(ë‚´ë¶€)ì´ Î»=0.0(ì™¸ë¶€)ë³´ë‹¤ 2ë°° ì„±ëŠ¥
2. **ê¸°ì–µì€ G ì¡°ì •**: í–‰ë™ ì§ì ‘ ì§€ì‹œê°€ ì•„ë‹ˆë¼ Expected Free Energy ìˆ˜ì •
3. **í›„íšŒëŠ” í•™ìŠµ ìì› ë°°ë¶„**: ì •ì±… ì§ì ‘ ìˆ˜ì • X, memory_gate/lr_boostì— ì—°ê²°
4. **Drift ì ì‘**: transition error spike â†’ recall ì–µì œ â†’ ì˜ëª»ëœ ê¸°ì–µ ì˜ì¡´ ë°©ì§€
5. **PC-gated Recovery**: ì™¸ë¶€ scoreê°€ ì•ˆì •í•´ë„ ë‚´ë¶€ residualì´ ë†’ìœ¼ë©´ ë³µê·€ ì§€ì—° (v5.14)
6. **E6 ì—°ì† ì œì–´**: Action EMAëŠ” lag ìœ ë°œ, Risk filter + hysteresisë§Œ ìœ ìµ
7. **E7 ì‹œê°„-ê¸°í•˜ ë¶„ì„**: Reactiveë§Œìœ¼ë¡œëŠ” TTC-short ìœ„í˜‘ì— ì·¨ì•½, TTC triggerê°€ ì˜ˆì¸¡ ì œì–´ë¡œ ì „í™˜
8. **E8 ë„ë©”ì¸ ì „ì´**: ìŠ¤íƒ ë²”ìš©ì„± í™•ì¸, TTCëŠ” ìœ„í˜‘ ì˜ˆì¸¡ ê°€ëŠ¥ì„±ì— ë¹„ë¡€í•´ ì •ë‹¹í™”
9. **MiniGrid ì™¸ë¶€ ê²€ì¦**: DIP ZONE(ì• ë§¤í•œ ì¶”ì ) í˜„ìƒì´ ì™¸ë¶€ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ì¬í˜„ë¨
10. **Phase G ì‹¤ì‹œê°„ í•™ìŠµ**: snnTorch + DA-STDPë§Œìœ¼ë¡œ ì‹¤ì‹œê°„ ë°˜ì‘ ê³¼ì œ(Chrome Dino) í•™ìŠµ ê°€ëŠ¥, 12ë§Œ ë‰´ëŸ° 53 steps/sec

### E6â†’E7â†’E8â†’MiniGrid ìµœì¢… ê²°ë¡ 

> **"We observe a regime-dependent phase diagram for preemptive TTC defense: TTC dominates in clearly non-pursuing (random) and clearly pursuing (strong) settings, but under an ambiguous pursuit regime (DIP ZONE) TTC over-triggers defense and increases opportunity-cost failures. This phase behavior is reproduced in an external benchmark (MiniGrid), demonstrating that regime-conditioned policy selection generalizes beyond custom environments."**

### í•œ ì¤„ ìš”ì•½

> "TTC ì„ ì œ ë°©ì–´ëŠ” ì¶”ì  ì‹ í˜¸ê°€ ëª…í™•í•  ë•Œë§Œ ìœ íš¨í•˜ë©°, ì• ë§¤í•œ ë ˆì§(DIP ZONE)ì—ì„œëŠ” RFê°€ ìš°ì›”í•˜ë‹¤. ì´ ìœ„ìƒì „ì´ëŠ” ë‚´ë¶€/ì™¸ë¶€ ë²¤ì¹˜ì—ì„œ ì¼ê´€ë˜ê²Œ ì¬í˜„ëœë‹¤."

---

## Transfer Learning: Causal Curiosity & Prior Suppression

MiniGrid í™˜ê²½ì—ì„œ ìƒˆë¡œìš´ ì¸ê³¼ ê·œì¹™ì„ ë°œê²¬í•˜ê³  í•™ìŠµí•˜ëŠ” ëŠ¥ë ¥ ê²€ì¦.

### Phase 5: Causal Curiosity (ì¸ê³¼ì  í˜¸ê¸°ì‹¬)

**ë¬¸ì œ**: Keyâ†’Door ê·œì¹™ì„ í•™ìŠµí•œ ë‡Œê°€ Switchâ†’Door (ìƒˆë¡œìš´ ì¸ê³¼ ê·œì¹™)ì„ í•™ìŠµí•  ìˆ˜ ìˆëŠ”ê°€?

**ë©”ì»¤ë‹ˆì¦˜**:
```python
# 1. Object interaction tracking
object_interactions = {}   # {(obj_type, color): count}

# 2. Causal position memory
causal_positions = {}      # {(x, y): {'effect': str, 'count': int}}

# 3. Causal surprise detection
if door_opened and not carrying_key:
    # Unexpected causal effect - record position
    causal_positions[agent_pos] = {'effect': 'door_opened', 'count': 1}
```

### Phase 6: Pattern Conflict Detection & Prior Suppression

**ë¬¸ì œ**: ê¸°ì¡´ Keyâ†’Door ì§€ì‹ì´ Switchâ†’Door í•™ìŠµì„ ë°©í•´í•¨ (Negative Transfer)

**ë©”ì»¤ë‹ˆì¦˜**:
```python
# 1. Pattern failure tracking
if action == TOGGLE and near_door and not door_opened:
    consecutive_door_failures += 1

# 2. Prior suppression
if consecutive_door_failures >= threshold:
    pattern_suppression['no_key_toggle'] += 0.2

# 3. G adjustment
G[TOGGLE] += suppression_weight * 2.0  # Make toggle less attractive
G[FORWARD] -= suppression_weight * 0.5  # Encourage exploration
```

### Transfer Test ê²°ê³¼

| Test | Fresh Agent | Transfer Agent |
|------|-------------|----------------|
| Cardâ†’Gate (Reskin) | 43% | 37% (3.5x speedup) |
| Switchâ†’Door (Rule Swap) | 27% | 43% (+13% learning) |

**í•µì‹¬ ë°œê²¬**:
1. **Causal Discovery Works**: ë‡Œê°€ ìƒˆë¡œìš´ ì¸ê³¼ ê´€ê³„ ë°œê²¬ (2 discoveries per run)
2. **Prior Suppression Helps**: Transfer learning +13% (ì´ì „ 0% ë˜ëŠ” ìŒìˆ˜)
3. **Plasticity Score**: 3/5 (ì´ì „ 1-2/5ì—ì„œ ê°œì„ )

### Phase 7: Directed Exploration (ë°©í–¥ì„± íƒìƒ‰)

**ë¬¸ì œ**: Switch ë°œê²¬ìœ¨ì´ 9-13%ë¡œ ë‚®ìŒ (ìš°ì—°ì— ì˜ì¡´)

**ë©”ì»¤ë‹ˆì¦˜**:
```python
# 1. Visit tracking
visited_positions = set()   # {(x, y), ...}
visit_counts = {}           # {(x, y): count}

# 2. Frontier detection (low-visit adjacent positions)
for pos in nearby_positions:
    if visit_counts.get(pos, 0) < 2:
        frontier_scores[pos] = novelty / distance

# 3. Directed movement to frontier
if should_explore:  # door_locked OR suppression_active OR stuck
    G[forward] -= bonus * alignment_to_frontier
```

**ê²°ê³¼**:
- Switch ë°œê²¬ìœ¨: **9% â†’ 19%** (2ë°° ì¦ê°€)
- Directed exploration moves: 625-778 per run
- Pattern conflicts detected: 8 (suppression í™œì„±í™”)

### Transfer Learning ë©”ì»¤ë‹ˆì¦˜ ìš”ì•½

| Phase | ë©”ì»¤ë‹ˆì¦˜ | íš¨ê³¼ |
|-------|---------|------|
| 5 | Causal Curiosity | ìƒˆ ì¸ê³¼ê´€ê³„ ë°œê²¬ (2-3 discoveries) |
| 6 | Prior Suppression | ì‹¤íŒ¨ íŒ¨í„´ ì–µì œ (+13% transfer learning) |
| 7 | Directed Exploration | Switch ë°œê²¬ìœ¨ 2ë°° ì¦ê°€ (9% â†’ 19%) |

### íŒŒì¼ êµ¬ì¡°

```
backend/benchmark/survival_minigrid.py:
â”œâ”€â”€ SurvivalGenesisBrain
â”‚   â”œâ”€â”€ Phase 5: object_interactions, causal_positions
â”‚   â”œâ”€â”€ Phase 6: pattern_attempts, pattern_suppression
â”‚   â”œâ”€â”€ Phase 7: visited_positions, visit_counts
â”‚   â”œâ”€â”€ _compute_causal_curiosity_G()
â”‚   â”œâ”€â”€ _detect_causal_surprise()
â”‚   â”œâ”€â”€ _detect_pattern_conflict()
â”‚   â”œâ”€â”€ _compute_prior_suppression_G()
â”‚   â””â”€â”€ _compute_directed_exploration_G()
â”œâ”€â”€ CardGateEnv (Isomorphic Reskin)
â”œâ”€â”€ SwitchDoorEnv (Rule Swap)
â””â”€â”€ run_transfer_tests()
```

---

> "ë‡Œì˜ ê·¼ë³¸ì ì¸ ì‘ë™ ì›ë¦¬ê°€ ëª¨ë“  í–‰ìœ„, ìƒê°, ê°ì •ì˜ ê·¼ì›ì´ ë˜ì–´ì•¼ í•œë‹¤"
